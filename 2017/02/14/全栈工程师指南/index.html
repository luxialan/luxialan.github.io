<!DOCTYPE html><html lang="zh-Hans"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="每天进步一点点"><title>【深度学习】 读书笔记 捌 深度模型中的优化 | 水冼雪</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">【深度学习】 读书笔记 捌 深度模型中的优化</h1><a id="logo" href="/.">水冼雪</a><p class="description">抬头看天，低头赶路</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">【深度学习】 读书笔记 捌 深度模型中的优化</h1><div class="post-meta">Feb 14, 2017<span> | </span><span class="category"><a href="/categories/焚膏继晷/">焚膏继晷</a><a href="/categories/焚膏继晷/知识储备/">知识储备</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a href="/2017/02/14/全栈工程师指南/#comments" class="ds-thread-count cloud-tie-join-count"><span style="font-size: 15px; color: #6E7173;" class="join-count">0</span><span> 条参与</span></a><div class="post-content"><p>前馈神经网络对于机器学习的实践是极其重要的。<br><a id="more"></a></p>
<ul>
<li>卷积运算</li>
<li>动机</li>
<li>池化</li>
<li>卷积与池化作为一种无限强的先验</li>
<li>基本卷积函数的变体</li>
<li>结构化输出</li>
<li>数据类型</li>
<li>高效的卷积算法</li>
<li>随机或无监督的特征</li>
<li>卷积神经网络的神经科学基础</li>
<li>卷积神经网络与深度学习的历史</li>
</ul>
<p>##<br>卷积网络，也叫做卷积神经网络，是一种专门用来处理具有类似网格结构的数据的神经网络。卷积是一种特殊的线性运算。卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络。</p>
<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>卷积运算通过三个重要的思想来帮助改进机器学习系统：稀疏交互、参数共享、等变表示。<br>传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。参数矩阵的每一个独立的参数都描述了每一个输入单元与每一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。<br>这通过使得核的规模远小于输入的规模来实现。</p>
<h2 id="随机或无监督的特征"><a href="#随机或无监督的特征" class="headerlink" title="随机或无监督的特征"></a>随机或无监督的特征</h2><p>减少卷积网络训练成本的一种方式是使用那些不是通过有监督方式训练的特征。<br>有三种基本策略不通过有监督训练而得到卷积核。</p>
<ul>
<li>简单地随机初始化它们</li>
<li>手动设计它们</li>
<li>无监督的标准来学习核<h2 id="卷积神经网络与深度学习的历史"><a href="#卷积神经网络与深度学习的历史" class="headerlink" title="卷积神经网络与深度学习的历史"></a>卷积神经网络与深度学习的历史</h2>卷积网络在深度学习的历史中发挥了重要作用。它们是将研究大脑获得的深刻理解成功用于机器学习应用的关键例子。</li>
</ul>
<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><h2 id="卷积与池化未做一种无限强的先验"><a href="#卷积与池化未做一种无限强的先验" class="headerlink" title="卷积与池化未做一种无限强的先验"></a>卷积与池化未做一种无限强的先验</h2><p>我们看到数据之前我们认为什么样的模型是合理的信念。<br>先验被认为是强或者弱取决于先验中概率密度的集中程度。弱先验具有较高的熵值，例如方差很大的高斯分布，这样的先验允许数据对于参数的改变具有或多或少的自由性。强先验具有较低的熵值，例如方差很小的高斯分布，这样的先验在决定参数最终取值时起着更加积极的作用。<br>对于全连接网络的权值有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权值必须和它邻居的权值相等，但在 空间中改变。这个先验也要求除了那些处在隐藏单元空间连续的小的接受域以内的权值外，其余的权值都为零。使用池化也是一个无限强的先验：每一个单元都具有少量平移的不变性。<br>其中一个关键的洞察是卷积和池化可能导致欠拟合<br>另一个关键洞察是当我们比较卷积模型的统计学习表现时，指能以基准中的其他卷积模型作为比较的对象。</p>
<h2 id="基本卷积函数的变体"><a href="#基本卷积函数的变体" class="headerlink" title="基本卷积函数的变体"></a>基本卷积函数的变体</h2><p>学习和优化有什么不同<br>机器学习通常是简介的。在大多数机器学习问题中，我们关注定义于测试集上的，也可能是不可解的性能度量$P$。因此，我们只是简介地优化$P$。我们希望通过降低损失函数$J(\theta)$来提高$P$。这一点不同于纯优化最小化$J$本身。<br>$$J(\theta)=\mathtt{E}_{(x,y)~\hat{p}_data}L(f(x;\theta),y)$$<br>其中$L$是每个样本的损失函数，$f(x;\theta)$是输入是$x$时的预测输出，$\hat{p}_data$是经验分布。<br>通常，我们更希望最小化期望取自数据生成分布$p<em>data$，$$J^{*}(\theta)=\mathtt{E}</em>{(x,y)~\hat{p}_data}L(f(x;\theta),y)$$</p>
<h2 id="经验风险最小化"><a href="#经验风险最小化" class="headerlink" title="经验风险最小化"></a>经验风险最小化</h2><p>机器学习算法的目标是降低式(8.2)所示的期望泛化误差。这个数据量被称为风险(risk)。值得注意的是，该期望取自真实的数据分布$p<em>data$。<br>机器学习问题用训练集上的经验分布$\hat{p}(x,y)$替代真实分布$p(x,y)$。如此，我们最小化经验风险$$\mathtt{E}</em>{(x,y)~\hat{p}_data}L(f(x;\theta),y)=frac{1}{m}\$$<br>基于最小化如上平均训练误差的训练过程被称为经验风险最小化。<br>经验风险最小化容易过拟合。高容量的模型会简单地记住训练集。在很多情况下，经验风险最小化并非真的可行。最有效的现代优化算法是基于梯度下降的。在深度学习中我们很少使用经验风险最小化。</p>
<h2 id="替代损失函数和提前终止"><a href="#替代损失函数和提前终止" class="headerlink" title="替代损失函数和提前终止"></a>替代损失函数和提前终止</h2><h2 id="（小）批算法"><a href="#（小）批算法" class="headerlink" title="（小）批算法"></a>（小）批算法</h2><p>机器学习算法的目标函数通常可以分解成训练样本上的求和。机器学习优化算法通常使用整个损失函数中的一部分项去更新其参数。</p>
<h2 id="神经网络的优化挑战"><a href="#神经网络的优化挑战" class="headerlink" title="神经网络的优化挑战"></a>神经网络的优化挑战</h2><p>在训练神经网络时，我们肯定会遇到一般的非凸情况。即使是凸优化，也并非没有任何问题。在这一节中，我们会总结几个训练深度模型时会涉及到的主要挑战。</p>
<h3 id="8-2-1-病态"><a href="#8-2-1-病态" class="headerlink" title="8.2.1 病态"></a>8.2.1 病态</h3><p>海森矩阵$H$的病态。这是数值优化，凸优化或其他形式的优化中普遍存在的问题。<br>病态体现在随机梯度下降会“卡”在某些情况，此时即使很小的更新步长也会增加损失函数。</p>
<h3 id="8-2-2-局部极小值"><a href="#8-2-2-局部极小值" class="headerlink" title="8.2.2 局部极小值"></a>8.2.2 局部极小值</h3><h3 id="8-2-3-高原，鞍点和其他平坦区域"><a href="#8-2-3-高原，鞍点和其他平坦区域" class="headerlink" title="8.2.3 高原，鞍点和其他平坦区域"></a>8.2.3 高原，鞍点和其他平坦区域</h3><h3 id="8-2-4-悬崖和梯度爆炸"><a href="#8-2-4-悬崖和梯度爆炸" class="headerlink" title="8.2.4 悬崖和梯度爆炸"></a>8.2.4 悬崖和梯度爆炸</h3><p>多层神经网络通常有像悬崖一样的斜率较大区域，如图8.3所示。这是由于几个较大的权重相乘导致的。遇到斜率极大的悬崖结构时，梯度更新会很大程度地改变参数值，通常会跳过这类悬崖结构。但幸运的是可以使用启发式梯度截断来避免其主要缺点。</p>
<h3 id="8-2-5-长期相关性"><a href="#8-2-5-长期相关性" class="headerlink" title="8.2.5 长期相关性"></a>8.2.5 长期相关性</h3><p>反复使用相同的参数产生了尤为突出的困难。<br>重复与矩阵$W$相乘的路径。<br>当特征值$\lambda_i$不再$1$附近时，若在量级上大于1则会膨胀到很大；若小于1时则会收缩到很小。<br>收缩和膨胀的梯度问题是指计算图上的梯度也会因为$diag(\lambda)^{t}$，诱发梯度截断的悬崖结构便是膨胀梯度现象的一个例子。<br>从这个观点来看，</p>
<h2 id="8-3-基本算法"><a href="#8-3-基本算法" class="headerlink" title="8.3 基本算法"></a>8.3 基本算法</h2><h3 id="8-3-1-随机梯度下降"><a href="#8-3-1-随机梯度下降" class="headerlink" title="8.3.1 随机梯度下降"></a>8.3.1 随机梯度下降</h3><p>通过计算独立同分布地从数据生成分布中抽取的$m$个$minibatch$样本的梯度均值，我们可以得到梯度的无偏估计。在实践中，有必要随着时间的推移逐渐降低学习速率。<br>通常，就总训练时间和最终损失值而言，最优初始学习速率会高于大约迭代100步后输出最好效果的学习速率。<br>对于足够大的数据集，$SGD$可能会在处理整个训练集之前就收敛到最终测试集误差的某个固定容差范围内。<br>算法的收敛率，即当前损失函数超出最低可能损失的量。$batch$梯度下降在理论上比随机梯度下降有更好的收敛率。</p>
<h3 id="动量"><a href="#动量" class="headerlink" title="动量"></a>动量</h3><h3 id="Nesterov-动量"><a href="#Nesterov-动量" class="headerlink" title="Nesterov 动量"></a>Nesterov 动量</h3><h2 id="参数初始化策略"><a href="#参数初始化策略" class="headerlink" title="参数初始化策略"></a>参数初始化策略</h2><p>大多数初始化策略基于在神经网络初始化时实现一些很好的性质。我们没有很好地理解这些性质中的哪些会在学习开始进行后的哪些情况下得以保持。进一步的难点是，有些初始点从优化的观点看或许是有利的，但是从泛化的观点看是不利的。我们对于初始点如何影响泛化的理解是相当原始的，几乎没有提供如何选择初始点的任何指导。<br>也许完全确指的唯一特性是初始参数需要在不同单元间“破坏对称性”。<br>如果我们有和输出一样多的输入，我们可以使用$Gram-Schmidt$正交化于初始的权重矩阵，保证每个单元计算彼此非常不同的函数。在高纬空间上使用高熵分布来随机初始化，计算代价小而且不太可能分配单元计算彼此相同的函数。<br>我们几乎总是初始化模型的权重为高斯或均匀分布中随机抽取的值。初始分布的大小确实对优化过程的结果和网络泛化能力有很大的影响。<br>更大的初始权重具有更强的破坏对称性的作用，有助于避免冗余的单元。正则化和优化有非常不同的视角。优化视角建议权重应该足够大以成功传播信息，但是正则化希望其小一点。诸如随机梯度下降这类对权重较小的增量更新，趋于停止在更靠近的初始参数的区域。</p>
<h2 id="自适应学习率的算法"><a href="#自适应学习率的算法" class="headerlink" title="自适应学习率的算法"></a>自适应学习率的算法</h2><p>学习率是难以设置的超参数之一，因为它对模型的性能有显著的影响。<br>$delta-bar-delta算法$，该方法基于一个很简单的想法，如果损失对于某个给定模型参数的偏导保持相同的符号，那么学习速率应该增加。</p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>$AdaGrad$算法。放缩每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习速率，而具有小偏导的参数在学习速率上有较小的下降。</p>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>$RMSProp$算法修改$AdaGrad$以在非凸设定下效果更好。</p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>$Adam$</p>
<h2 id="选择正确的优化算法"><a href="#选择正确的优化算法" class="headerlink" title="选择正确的优化算法"></a>选择正确的优化算法</h2><h2 id="二阶近似方法"><a href="#二阶近似方法" class="headerlink" title="二阶近似方法"></a>二阶近似方法</h2><h3 id="牛顿方法"><a href="#牛顿方法" class="headerlink" title="牛顿方法"></a>牛顿方法</h3><h3 id="共轭梯度"><a href="#共轭梯度" class="headerlink" title="共轭梯度"></a>共轭梯度</h3><h2 id="优化技巧和元算法"><a href="#优化技巧和元算法" class="headerlink" title="优化技巧和元算法"></a>优化技巧和元算法</h2><h3 id="batch-normalization"><a href="#batch-normalization" class="headerlink" title="batch normalization"></a>batch normalization</h3><h3 id="坐标下降"><a href="#坐标下降" class="headerlink" title="坐标下降"></a>坐标下降</h3><h3 id="Polyak-平均"><a href="#Polyak-平均" class="headerlink" title="Polyak 平均"></a>Polyak 平均</h3></div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://luxialan.com/2017/02/14/全栈工程师指南/" data-id="cj5xhmtqo001npkwewsbvtlv6" class="article-share-link">分享</a><div class="tags"><a href="/tags/Book/">Book</a></div><div class="post-nav"><a href="/2017/02/14/【深度学习】-读书笔记-伍-机器学习基础/" class="pre">【深度学习】 读书笔记 伍 机器学习基础</a><a href="/2017/02/14/2017第一谈/" class="next">【深度学习】 读书笔记 拾一 深度模型中的优化</a></div><div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div><script>var cloudTieConfig = {
  url: document.location.href,
  productKey: "true",
  target: "cloud-tie-wrapper"
};</script><script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://luxialan.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/焚膏继晷/">焚膏继晷</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/焚膏继晷/技术管理/">技术管理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/焚膏继晷/知识储备/">知识储备</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/焚膏继晷/经济金融/">经济金融</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/随便侃侃/">随便侃侃</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/随便侃侃/周记/">周记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随便侃侃/总结/">总结</a></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Book/" style="font-size: 15px;">Book</a> <a href="/tags/Economy-Finance/" style="font-size: 15px;">Economy/Finance</a> <a href="/tags/Gossip/" style="font-size: 15px;">Gossip</a> <a href="/tags/Note/" style="font-size: 15px;">Note</a> <a href="/tags/Technique/" style="font-size: 15px;">Technique</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/08/02/【金钱有术】-读后感/">【金钱有术】 读后感</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/01/【量化投资方法论】-笔记/">【量化投资方法论】 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/30/【第二周】-有朋自远方来/">【第二周】 有朋自远方来</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/29/【量化投资与当前市场】-笔记/">【量化投资与当前市场】 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/18/【证券分析】-读书笔记/">【证券分析】 读书笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/12/【第一周】-逆水行舟/">【第一周】 逆水行舟</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/04/前端工程师指南/">前端工程师指南</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/02/研究生这三年/">研究生这三年</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/19/【程序员健康指南】-读书笔记/">【程序员健康指南】 读书笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/07/【量化交易-如何建立自己的算法交易】-读书笔记/">【量化交易 如何建立自己的算法交易】 读书笔记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2017 <a href="/." rel="nofollow">水冼雪.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>